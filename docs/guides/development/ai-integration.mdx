---
path: 'docs/guides/development/ai-integration.mdx'
title: 'AI Integration & Local Knowledge Core'
description: 'Guide to the high-performance local MPC Server and RAG pipeline.'
order: 50
icon: 'mdi:robot'
author: 'admin'
created: '2026-02-07'
updated: '2026-02-24'
tags:
  - 'ai'
  - 'mcp'
  - 'rag'
  - 'npu'
  - 'lancedb'
---

# AI Integration & Local Knowledge Core (MPC)

SveltyCMS 2026 leverages a high-performance **Model Context Protocol (MCP)** server to provide AI agents with deep, real-time project knowledge.

> [!TIP]
> **Status:** Active. Your local knowledge core is running at `D:\mpc` using **LanceDB** and **DirectML (NPU/iGPU)** acceleration via **HuggingFace Transformers v3**.

## Architecture

### 1. The Local Knowledge Core (D:\mpc)

This service acts as the "Long-term Memory" for SveltyCMS:

- **DirectML / NPU Indexing:** Uses the **Intel NPU (AI Boost)** and **iGPU** via DirectML to handle embeddings, keeping your RTX 5080 and CPU cores free.
- **Deep Implementation Knowledge:** Indexes `src/`, `docs/`, and `package.json` expert patterns.
- **Expert Discovery:** Automatically syncs with `llms-full.txt` from Svelte 5, Skeleton UI, and Valibot.

### 2. Private Inference (NVIDIA 5080)

While the _Knowledge_ is retrieved via the NPU, the _Inference_ (Generation) remains on your **RTX 5080** via **Ollama**:

- **Speed:** Maximum VRAM for large context windows.
- **Privacy:** No code or data ever leaves your local D: drive.

### 3. Hardware Workload Split

| Task              | Processor            | Reason                                     |
| :---------------- | :------------------- | :----------------------------------------- |
| **Indexing/RAG**  | **Intel NPU / iGPU** | Offloads AI math from main processors.     |
| **LLM Inference** | **NVIDIA 5080**      | Maximum throughput for complex reasoning.  |
| **Compiling**     | **Intel 275HX CPU**  | 24 cores optimized for Vite/Svelte builds. |

## Usage for Developers

### Background Sync

To keep the knowledge base always up-to-date, run the sync service:

```powershell
cd D:\mpc; bun run sync-service.ts
```

### Editor Integration

Connect your AI assistant (Zed, Windsurf, or Antigravity) by adding the following to your context server settings:

```json
"command": "bun",
"args": ["run", "D:/mpc/index.ts"]
```

## Troubleshooting: NPU Monitoring

If Windows 11 Task Manager shows **0% NPU usage** during indexing:

- **Intel AI Boost:** For smaller models (BGE-Small), the DirectML driver often intelligently routes work to the **iGPU (GPU 1)** instead of the NPU. This is normal and still offloads work from your primary GPU.
- **Performance Verification:**
  - **CPU (Native):** ~9622ms
  - **DirectML (NPU/iGPU):** **~8965ms** (Hardware engaged)
- **Manual Check:** You can re-run the benchmark at `D:\mpc\benchmark-build.ts` at any time.
